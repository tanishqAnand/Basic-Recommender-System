This folder contains the KNN algorithm for recommendation system. Unsupervised nearest neighbors is the foundation of many other learning methods,
notably manifold learning and spectral clustering. 
The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these.
The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). 
The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice.
Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree).

The classes in sklearn.neighbors can handle either NumPy arrays or scipy.sparse matrices as input.
For dense matrices, a large number of possible distance metrics are supported. 
For sparse matrices, arbitrary Minkowski metrics are supported for searches.
